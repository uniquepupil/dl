{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4317279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b4ed3b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 62\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('as', 5), ('of', 6), ('machine', 7), ('supervised', 8), ('and', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Data: list of sentences about deep learning\n",
    "data = [\n",
    "    \"Deep learning also known as deep structured learning\",\n",
    "    \"is part of a broader family of machine learning methods based\",\n",
    "    \"on artificial neural networks with representation learning\",\n",
    "    \"Learning can be supervised, semi-supervised or unsupervised\",\n",
    "    \"Deep-learning architectures such as deep neural networks\",\n",
    "    \"deep belief networks, deep reinforcement learning\",\n",
    "    \"recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation\",\n",
    "    \"where they have produced results comparable to and in some cases surpassing human expert performance\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and build vocabulary\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0  # Add padding for sequence compatibility\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# Convert sentences to sequences of word IDs\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100  # Embedding vector size\n",
    "window_size = 2  # Context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58049628",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Context-Target Pairs:\n",
      "Context (X): ['learning', 'also'] -> Target (Y): deep\n",
      "Context (X): ['deep', 'also', 'known'] -> Target (Y): learning\n",
      "Context (X): ['deep', 'learning', 'known', 'as'] -> Target (Y): also\n",
      "Context (X): ['learning', 'also', 'as', 'deep'] -> Target (Y): known\n",
      "Context (X): ['also', 'known', 'deep', 'structured'] -> Target (Y): as\n",
      "Context (X): ['known', 'as', 'structured', 'learning'] -> Target (Y): deep\n"
     ]
    }
   ],
   "source": [
    "# Function to generate context-target pairs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            # Get context words\n",
    "            context_words = [\n",
    "                words[i]\n",
    "                for i in range(index - window_size, index + window_size + 1)\n",
    "                if 0 <= i < sentence_length and i != index\n",
    "            ]\n",
    "            # Pad context to fixed length\n",
    "            x = pad_sequences([context_words], maxlen=context_length)[0]\n",
    "            y = to_categorical(word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "# Display a few context-target pairs\n",
    "print(\"\\nSample Context-Target Pairs:\")\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "    print('Context (X):', [id2word[w] for w in x if w != 0], '-> Target (Y):', id2word[np.argmax(y)])\n",
    "    if i == 5:  # Show only the first 5 pairs\n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43bbd775",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 100)            6200      \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                6262      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12462 (48.68 KB)\n",
      "Trainable params: 12462 (48.68 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the CBOW model\n",
    "cbow = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2),\n",
    "    Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396a5163",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adity\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch: 10 \tLoss: 302.17851543426514\n",
      "Epoch: 20 \tLoss: 250.64994937181473\n",
      "Epoch: 30 \tLoss: 209.60554061830044\n",
      "Epoch: 40 \tLoss: 177.3139235600829\n",
      "Epoch: 50 \tLoss: 150.38395673036575\n"
     ]
    }
   ],
   "source": [
    "# Train the CBOW model\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = 0\n",
    "    for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "        loss += cbow.train_on_batch(x.reshape(1, -1), y.reshape(1, -1))\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', epoch, '\\tLoss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b577304",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding matrix shape: (61, 100)\n",
      "\n",
      "Word Embeddings (Sample):\n",
      "                0         1         2         3         4         5   \\\n",
      "learning -0.261622  0.175840  0.327796 -0.032309  0.418981  0.061149   \n",
      "deep      0.817920 -0.018986  0.653333  0.150152 -0.117192 -0.162195   \n",
      "networks -0.248330  0.188581 -0.131515  0.175582  0.212774  0.380598   \n",
      "neural    0.244306 -0.157685  0.272999  0.572180  0.591226 -0.453447   \n",
      "as        0.119288 -0.057155  0.406259 -0.054688 -0.027695  0.588162   \n",
      "\n",
      "                6         7         8         9   ...        90        91  \\\n",
      "learning  0.296201  0.049553  0.018468 -0.026031  ...  0.334597 -0.286660   \n",
      "deep      0.579300  0.282415  0.114659 -0.087520  ...  0.495792  0.215134   \n",
      "networks -0.004691 -0.450504  0.579308 -0.119742  ... -0.440219 -0.023278   \n",
      "neural    0.641994  0.288245  0.138223  0.005732  ... -0.356352 -0.151586   \n",
      "as        0.217130  0.040373 -0.159168  0.029755  ...  0.049202 -0.113313   \n",
      "\n",
      "                92        93        94        95        96        97  \\\n",
      "learning -0.619846  0.151418  0.597741 -0.488457  0.402705 -0.437107   \n",
      "deep      0.564213  0.298000 -0.186095  0.257668 -0.492021 -0.121594   \n",
      "networks  0.232079 -0.799146  0.400974  0.057622 -0.028571  0.245316   \n",
      "neural    0.040781 -0.430443  0.504034 -0.158938 -0.426294  0.531898   \n",
      "as       -0.403788  0.269154  0.415265  0.125155  0.394716 -0.415574   \n",
      "\n",
      "                98        99  \n",
      "learning -0.192004  0.539441  \n",
      "deep      0.068696 -0.010130  \n",
      "networks -0.118367 -0.145583  \n",
      "neural   -0.208516 -0.632684  \n",
      "as       -0.069571  0.606171  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned word embeddings\n",
    "embedding_weights = cbow.get_weights()[0][1:]  # Exclude the padding index\n",
    "print(\"\\nEmbedding matrix shape:\", embedding_weights.shape)\n",
    "\n",
    "# Display word embeddings as a DataFrame\n",
    "print(\"\\nWord Embeddings (Sample):\")\n",
    "embedding_df = pd.DataFrame(embedding_weights, index=[id2word[i] for i in range(1, vocab_size)])\n",
    "print(embedding_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12bc3dda",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance matrix shape: (61, 61)\n",
      "\n",
      "Similar Words: {'deep': ['with', 'representation', 'recurrent', 'convolutional', 'known'], 'unsupervised': ['semi', 'or', 'can', 'be', 'based']}\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise Euclidean distances between embeddings\n",
    "distance_matrix = euclidean_distances(embedding_weights)\n",
    "print(\"\\nDistance matrix shape:\", distance_matrix.shape)\n",
    "\n",
    "# Find and display similar words for specific search terms\n",
    "similar_words = {\n",
    "    search_term: [\n",
    "        id2word[idx + 1] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1:6]\n",
    "    ]\n",
    "    for search_term in ['deep', 'unsupervised']\n",
    "}\n",
    "print(\"\\nSimilar Words:\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdab992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
